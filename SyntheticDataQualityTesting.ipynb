{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers scipy ftfy accelerate torch open_clip_torch"
      ],
      "metadata": {
        "id": "dcPrLuT28ZsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# diffusion tti model optimized for limited resources\n",
        "sd_pipe = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
        "\n",
        "# ResNet50 classification model\n",
        "weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "preprocess = weights.transforms() # preprocessing transformation\n",
        "classifier = resnet50(weights=weights)\n",
        "classifier.eval().to(device)"
      ],
      "metadata": {
        "id": "Q57iRV6P7qx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation by ResNet50\n",
        "def evaluate(tti, labels, n=1):\n",
        "    overall_agg = 0\n",
        "    for label in labels:\n",
        "        class_agg = 0\n",
        "        for _ in range(n):\n",
        "            # generate image from prompt\n",
        "            img = tti(label)\n",
        "\n",
        "            # preprocess image and input into ResNet50\n",
        "            batch = preprocess(img).unsqueeze(0).to(device)\n",
        "            prediction = classifier(batch).squeeze(0).softmax(0)\n",
        "\n",
        "            # obtain top category and corresponding score\n",
        "            class_id = prediction.argmax().item()\n",
        "            score = prediction[class_id].item()\n",
        "            category_name = weights.meta[\"categories\"][class_id]\n",
        "\n",
        "            class_agg += score # update class aggregate score\n",
        "            overall_agg += score # update overall aggregate score\n",
        "        print(f\"Prompt: {label} | Predicted category: {category_name} | Average score: {100 * class_agg / n:.1f}%\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(f\"Iterations per class: {n} | Average score: {100 * overall_agg / (len(labels) * n):.1f}%\")"
      ],
      "metadata": {
        "id": "L0M9seKngg2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 classes from ImageNet1K\n",
        "imagenet_classes = [\n",
        "    \"tench, Tinca tinca\",\n",
        "    \"goldfish, Carassius auratus\",\n",
        "    \"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\",\n",
        "    \"tiger shark, Galeocerdo cuvieri\",\n",
        "    \"hammerhead, hammerhead shark\",\n",
        "    \"electric ray, crampfish, numbfish, torpedo\",\n",
        "    \"stingray\",\n",
        "    \"cock\",\n",
        "    \"hen\",\n",
        "    \"ostrich, Struthio camelus\"\n",
        "]"
      ],
      "metadata": {
        "id": "ioJFtI2ZuMDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: use class name as prompt\n",
        "def classNameGeneration(label):\n",
        "    return sd_pipe(label).images[0]"
      ],
      "metadata": {
        "id": "S05j-hiz8fl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(classNameGeneration, imagenet_classes)"
      ],
      "metadata": {
        "id": "ohhw4rsG3qkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: use template \"A photo of a <class name>\"\n",
        "def classNameTemplateGeneration(label):\n",
        "    return sd_pipe(f\"A photo of a {label}\").images[0]"
      ],
      "metadata": {
        "id": "pluRbeF6HlrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(classNameTemplateGeneration, imagenet_classes)"
      ],
      "metadata": {
        "id": "SQ2nrryn3uaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# download and store images from imagenet-sample-images repo\n",
        "\n",
        "# create directory to store images\n",
        "if not os.path.exists(\"imagenet_samples\"):\n",
        "    os.makedirs(\"imagenet_samples\")\n",
        "\n",
        "# download images and store in imagenet_samples directory\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01440764_tench.JPEG?raw=true -O imagenet_samples/tench.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01443537_goldfish.JPEG?raw=true -O imagenet_samples/goldfish.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01484850_great_white_shark.JPEG?raw=true -O imagenet_samples/great_white_shark.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01491361_tiger_shark.JPEG?raw=true -O imagenet_samples/tiger_shark.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01494475_hammerhead.JPEG?raw=true -O imagenet_samples/hammerhead.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01496331_electric_ray.JPEG?raw=true -O imagenet_samples/electric_ray.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01498041_stingray.JPEG?raw=true -O imagenet_samples/stingray.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01514668_cock.JPEG?raw=true -O imagenet_samples/cock.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01514859_hen.JPEG?raw=true -O imagenet_samples/hen.jpg\n",
        "!wget -q https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01518878_ostrich.JPEG?raw=true -O imagenet_samples/ostrich.jpg\n",
        "\n",
        "# list of image names\n",
        "img_names = [\"tench\", \"goldfish\", \"great_white_shark\", \"tiger_shark\", \"hammerhead\",\n",
        "             \"electric_ray\", \"stingray\", \"cock\", \"hen\", \"ostrich\"]"
      ],
      "metadata": {
        "id": "HE2DpZd-nRzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 3: Use contrastive captioning (CoCa) to generate prompts from ImageNet images\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "\n",
        "# CoCa model and preprocessing transformation\n",
        "CoCa, _, transform = open_clip.create_model_and_transforms(\n",
        "    model_name=\"coca_ViT-L-14\",\n",
        "    pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
        ")\n",
        "CoCa.to(device)\n",
        "\n",
        "# generate captions with CoCa\n",
        "captions = []\n",
        "for name in img_names:\n",
        "    # get image from imagenet_samples directory\n",
        "    path = \"/content/imagenet_samples/\" + name + \".jpg\"\n",
        "    im = Image.open(path).convert(\"RGB\")\n",
        "    im = transform(im).unsqueeze(0).to(device)\n",
        "\n",
        "    # generate caption for image\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        generated = CoCa.generate(im)\n",
        "\n",
        "    captions.append(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
      ],
      "metadata": {
        "id": "gl27QyYOuQaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(classNameGeneration, captions)"
      ],
      "metadata": {
        "id": "s2UaW_-dxjTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from diffusers import StableDiffusionImageVariationPipeline\n",
        "\n",
        "# image variation model\n",
        "sdiv_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n",
        "  \"lambdalabs/sd-image-variations-diffusers\",\n",
        "  revision=\"v2.0\",\n",
        ")\n",
        "sdiv_pipe = sdiv_pipe.to(device)\n",
        "\n",
        "# preprocessing transformation\n",
        "tform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize(\n",
        "          (224, 224),\n",
        "          interpolation=transforms.InterpolationMode.BICUBIC,\n",
        "          antialias=False,\n",
        "          ),\n",
        "      transforms.Normalize(\n",
        "        [0.48145466, 0.4578275, 0.40821073],\n",
        "        [0.26862954, 0.26130258, 0.27577711]),\n",
        "  ])"
      ],
      "metadata": {
        "id": "un-s-vYq3zp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 4: Image Variation\n",
        "from PIL import Image\n",
        "\n",
        "N = 1 # iterations per image\n",
        "\n",
        "agg = 0\n",
        "for name in img_names:\n",
        "    img_agg = 0\n",
        "    for _ in range(N):\n",
        "        # get image from imagenet_samples directory\n",
        "        path = \"/content/imagenet_samples/\" + name + \".jpg\"\n",
        "        im = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # apply image variation\n",
        "        im = tform(im).unsqueeze(0).to(device)\n",
        "        img = sdiv_pipe(im, guidance_scale=3).images[0]\n",
        "\n",
        "        # preprocess varied image and input into ResNet50\n",
        "        batch = preprocess(img).unsqueeze(0).to(device)\n",
        "        prediction = classifier(batch).squeeze(0).softmax(0)\n",
        "\n",
        "        # obtain top category and corresponding score\n",
        "        class_id = prediction.argmax().item()\n",
        "        score = prediction[class_id].item()\n",
        "        category_name = weights.meta[\"categories\"][class_id]\n",
        "\n",
        "        img_agg += score # update image-specific aggregate score\n",
        "        agg += score # update overall aggregate score\n",
        "    print(f\"Pre-varied image: {name} | Predicted category: {category_name} | Score: {100 * img_agg / N:.1f}%\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Iterations per class: {N} | Average score: {100 * agg / (len(img_names) * N):.1f}%\")"
      ],
      "metadata": {
        "id": "ZB6IQkhTO3LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes NSFW filter\n",
        "sd_pipe.safety_checker = lambda images, clip_input: (images, [False])"
      ],
      "metadata": {
        "id": "O2ORvkzyKScD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}